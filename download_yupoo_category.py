#!/usr/bin/env python3
"""
Script para descargar im√°genes de productos desde Yupoo por categor√≠a
Organiza las im√°genes en: Categor√≠a/P√°gina/Producto/imagen.jpg
"""

import requests
from bs4 import BeautifulSoup
from pathlib import Path
import time
import re
import argparse
from urllib.parse import urljoin, urlparse

# Configuraci√≥n por defecto (puede ser sobrescrita por argumentos)
BASE_URL = "https://yitian333.x.yupoo.com/categories/4135412"
CATEGORY_NAME = None  # None = extraer autom√°ticamente, o especifica manualmente: "TrapstarÁ≥ªÂàó"
START_PAGE = 1
END_PAGE = 2
MAX_RETRIES = 3
DELAY_BETWEEN_REQUESTS = 0.5
DELAY_BETWEEN_IMAGES = 0.3
PASSWORD = None  # Contrase√±a para p√°ginas protegidas (opcional)

def sanitize_filename(filename):
    """Limpia el nombre de archivo solo de caracteres problem√°ticos del sistema de archivos"""
    filename = str(filename)
    # Solo reemplaza caracteres que causan problemas en sistemas de archivos
    filename = filename.replace('/', '-').replace('\\', '-')
    filename = filename.replace('\0', '')
    # Limita la longitud solo si es extremadamente larga
    if len(filename) > 250:
        filename = filename[:250]
    return filename.strip()

def is_password_protected(soup):
    """Detecta si la p√°gina requiere contrase√±a"""
    page_text = soup.get_text().lower()
    html_str = str(soup).lower()
    return (
        'indexlock' in html_str or
        'encrypted' in page_text or
        'ËØ∑ËæìÂÖ•ÂØÜÁ†Å' in page_text or
        'enter password' in page_text or
        soup.find('div', class_=lambda x: x and 'indexlock' in str(x)) is not None
    )

def authenticate_if_needed(session, base_url, password=None):
    """
    Autentica en una p√°gina protegida si es necesario.
    
    Args:
        session: requests.Session object
        base_url: URL base de la categor√≠a
        password: Contrase√±a para p√°ginas protegidas (opcional)
    
    Returns:
        bool: True si la autenticaci√≥n fue exitosa o no era necesaria, False si fall√≥
    """
    if not password:
        return True  # No hay contrase√±a, asumir que no es necesaria
    
    try:
        # Hacer una petici√≥n inicial para verificar si necesita autenticaci√≥n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': base_url,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        }
        
        response = session.get(f"{base_url}?page=1", headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Si no requiere contrase√±a, retornar True
        if not is_password_protected(soup):
            return True
        
        # Extraer el dominio (OWNER) de la URL
        parsed = urlparse(base_url)
        domain_parts = parsed.netloc.split('.')
        if len(domain_parts) >= 2:
            owner = domain_parts[0]  # ej: wholesale4shoesbags de wholesale4shoesbags.x.yupoo.com
        else:
            owner = domain_parts[0]
        
        # Construir URL de autenticaci√≥n
        # API_ORIGIN es relativo: '/api', as√≠ que construimos la URL completa
        api_origin = f"{parsed.scheme}://{parsed.netloc}/api"
        auth_url = f"{api_origin}/web/users/{owner}?password={password}"
        
        # Hacer petici√≥n de autenticaci√≥n
        auth_response = session.get(auth_url, headers=headers, timeout=10)
        auth_response.raise_for_status()
        
        # Verificar si la autenticaci√≥n fue exitosa
        try:
            auth_data = auth_response.json()
            if auth_data.get('data', {}).get('passwordValid'):
                # Establecer cookie manualmente si no se estableci√≥ autom√°ticamente
                # La cookie se llama 'indexlockcode' seg√∫n el c√≥digo JavaScript
                # Usar dominio .x.yupoo.com para que funcione en todos los subdominios
                session.cookies.set('indexlockcode', password, domain='.x.yupoo.com', path='/')
                
                # Verificar que ahora podemos acceder al contenido
                test_response = session.get(f"{base_url}?page=1", headers=headers, timeout=10)
                test_soup = BeautifulSoup(test_response.text, 'html.parser')
                if not is_password_protected(test_soup):
                    return True
        except (ValueError, KeyError) as e:
            # Si no es JSON, verificar directamente si podemos acceder
            # Pero primero establecer la cookie
            # Usar dominio .x.yupoo.com para que funcione en todos los subdominios
            session.cookies.set('indexlockcode', password, domain='.x.yupoo.com', path='/')
            
            test_response = session.get(f"{base_url}?page=1", headers=headers, timeout=10)
            test_soup = BeautifulSoup(test_response.text, 'html.parser')
            if not is_password_protected(test_soup):
                return True
        
        return False
        
    except Exception as e:
        print(f"‚ö† Error en autenticaci√≥n: {e}")
        return False

def extract_category_name(soup, base_url):
    """Extrae el nombre de la categor√≠a desde el HTML respetando el nombre original"""
    # Extraer ID de categor√≠a de la URL para b√∫squedas m√°s precisas
    category_id = None
    if '/categories/' in base_url:
        match = re.search(r'/categories/(\d+)', base_url)
        if match:
            category_id = match.group(1)
    
    # Lista de nombres inv√°lidos a filtrar
    invalid_names = ['ÁÆÄ‰Ωì‰∏≠Êñá', 'english', 'ÁπÅÈ´î‰∏≠Êñá', 'espa√±ol', 'portugues', 'Fran√ßais', 
                     'Deutsch', '–†—É—Å—Å–∫–∏–π', 'ÁôªÂΩï', 'Ê≥®ÂÜå', 'Home', 'All categories', 
                     'Yupoo', 'search', 'QR code']
    
    # M√©todo 1: Buscar en el texto que dice "ÂàÜÁ±ª"XÁ≥ªÂàó"‰∏ãÁöÑÁõ∏ÂÜå" o "ÂàÜÁ±ª"X"‰∏ãÁöÑÁõ∏ÂÜå" (M√ÅS CONFIABLE)
    page_text = soup.get_text()
    # Buscar patr√≥n: ÂàÜÁ±ª"nombre"‰∏ãÁöÑÁõ∏ÂÜå (puede tener Á≥ªÂàó o no)
    match = re.search(r'ÂàÜÁ±ª["\']([^"\']+)["\']‰∏ãÁöÑÁõ∏ÂÜå', page_text)
    if match:
        category_name = match.group(1).strip()
        # Filtrar nombres inv√°lidos
        if category_name and category_name not in invalid_names:
            return category_name
    
    # Tambi√©n buscar en el t√≠tulo de la p√°gina
    title_tag = soup.find('title')
    if title_tag:
        title_text = title_tag.get_text()
        match = re.search(r'ÂàÜÁ±ª["\']([^"\']+)["\']‰∏ãÁöÑÁõ∏ÂÜå', title_text)
        if match:
            category_name = match.group(1).strip()
            if category_name and category_name not in invalid_names:
                return category_name
    
    # M√©todo 2: Buscar en breadcrumbs - solo enlaces que apunten a esta categor√≠a espec√≠fica
    if category_id:
        category_links = soup.find_all('a', href=lambda x: x and f'/categories/{category_id}' in str(x))
        for link in category_links:
            text = link.text.strip()
            href = link.get('href', '')
            # Filtrar enlaces de idioma (tienen par√°metros ?page=1 o son solo idiomas)
            # Y filtrar nombres de idioma y navegaci√≥n
            is_language_link = (text in invalid_names or 
                               text.lower() in ['english', 'ÁÆÄ‰Ωì‰∏≠Êñá', 'ÁπÅÈ´î‰∏≠Êñá', 'espa√±ol', 'portugues', 
                                               'fran√ßais', 'deutsch', '—Ä—É—Å—Å–∫–∏–π'] or
                               '?page=' in href or
                               len(text) < 3)
            if text and not is_language_link and text not in invalid_names:
                # Aceptar cualquier nombre que no sea de idioma, no necesita tener "Á≥ªÂàó"
                return text
    
    # M√©todo 3: Buscar en t√≠tulos h1, h2 que contengan "Á≥ªÂàó"
    for tag in ['h1', 'h2', 'h3']:
        titles = soup.find_all(tag)
        for title in titles:
            text = title.text.strip()
            if 'Á≥ªÂàó' in text:
                # Extraer la parte completa que contiene "Á≥ªÂàó"
                match = re.search(r'([^\s]+Á≥ªÂàó)', text)
                if match:
                    category_name = match.group(1)
                    # Filtrar nombres inv√°lidos
                    if category_name not in invalid_names:
                        return category_name
    
    # M√©todo 4: Buscar en la lista de categor√≠as del men√∫ lateral
    if category_id:
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            # Buscar enlaces que apunten exactamente a esta categor√≠a (sin par√°metros de idioma)
            if f'/categories/{category_id}' in href and '?page=' not in href:
                text = link.text.strip()
                # Filtrar nombres inv√°lidos y de idioma
                is_language = text.lower() in ['english', 'ÁÆÄ‰Ωì‰∏≠Êñá', 'ÁπÅÈ´î‰∏≠Êñá', 'espa√±ol', 'portugues', 
                                               'fran√ßais', 'deutsch', '—Ä—É—Å—Å–∫–∏–π'] or text in invalid_names
                if text and not is_language and len(text) > 2:
                    return text
    
    # M√©todo 5: Buscar en el breadcrumb de navegaci√≥n (√∫ltimo recurso)
    breadcrumbs = soup.find_all(['nav', 'ol', 'ul'], class_=lambda x: x and 'breadcrumb' in str(x).lower())
    for breadcrumb in breadcrumbs:
        links = breadcrumb.find_all('a', href=lambda x: x and '/categories/' in x)
        for link in links:
            text = link.text.strip()
            # Filtrar nombres inv√°lidos
            if text and text not in invalid_names and 'Á≥ªÂàó' in text:
                return text
    
    return None

def extract_products_from_page(soup, base_url):
    """Extrae informaci√≥n de productos de la p√°gina usando BeautifulSoup
    
    Returns:
        tuple: (lista de productos √∫nicos, n√∫mero de duplicados detectados, lista de duplicados con info)
    """
    products = []
    
    # Buscar todos los enlaces a /albums/
    album_links = soup.find_all('a', href=lambda x: x and '/albums/' in x)
    
    seen_album_ids = set()  # Usar ID del √°lbum para evitar duplicados
    seen_urls = set()
    
    for link in album_links:
        href = link.get('href', '')
        if not href:
            continue
        
        # Filtrar enlaces que no son productos (navegaci√≥n, etc.)
        # Los productos tienen par√°metros como ?uid=1&isSubCate=false
        if '?uid=' not in href and '&isSubCate=' not in href:
            # Puede ser un producto, pero verificar que no sea navegaci√≥n
            parent = link.find_parent(['nav', 'header', 'footer'])
            if parent:
                continue
        
        # Extraer ID del √°lbum de la URL
        album_id_match = re.search(r'/albums/(\d+)', href)
        if not album_id_match:
            continue
        
        album_id = album_id_match.group(1)
        
        # Evitar duplicados por ID
        if album_id in seen_album_ids:
            continue
        
        # Construir URL completa
        if href.startswith('/'):
            full_url = urljoin(base_url, href)
        elif href.startswith('http'):
            full_url = href
        else:
            continue
        
        # Evitar duplicados por URL completa
        if full_url in seen_urls:
            continue
        
        # El texto del enlace suele ser solo el n√∫mero de fotos (ej: "25")
        # Necesitamos buscar el nombre real en el contenedor padre
        product_name = None
        parent = link.find_parent(['div', 'article', 'section', 'li'])
        
        if parent:
            # Obtener todo el texto del contenedor
            container_text = parent.get_text(separator='|', strip=True)
            parts = [p.strip() for p in container_text.split('|') if p.strip()]
            
            # CORRECCI√ìN: Aceptar productos nombrados solo con n√∫meros
            # El formato suele ser: "n√∫mero_fotos|nombre_producto"
            if len(parts) >= 2:
                # El primer elemento es el n√∫mero de fotos, el segundo el nombre del producto
                product_name = parts[1]
            elif len(parts) == 1:
                # Si solo hay un elemento y no es un n√∫mero muy peque√±o (1-99), usarlo
                if not (re.match(r'^\d{1,2}$', parts[0])):
                    product_name = parts[0]
            
            # Si no encontramos con el m√©todo anterior, intentar b√∫squeda en l√≠neas
            if not product_name:
                lines = [text_line.strip() for text_line in container_text.replace('|', '\n').split('\n') if text_line.strip()]
                for line in reversed(lines):  # Empezar desde el final
                    if line and len(line) > 0:
                        # Ignorar solo n√∫meros muy peque√±os (1-99) que son probablemente conteos
                        invalid_names = ['ÁôªÂΩï', 'Ê≥®ÂÜå', 'Home', 'album', 'All categories', 'Yupoo', 'search', 'QR code']
                        is_small_number = re.match(r'^\d{1,2}$', line)
                        if (line not in invalid_names and 
                            not line.startswith('http') and 
                            not is_small_number):
                            product_name = line
                            break
            
            # Si no encontramos, buscar en headings dentro del contenedor
            if not product_name:
                headings = parent.find_all(['h2', 'h3', 'h4'])
                for heading in headings:
                    text = heading.get_text(strip=True)
                    if text and len(text) > 0:
                        link_text = link.get_text(strip=True)
                        is_small_number = re.match(r'^\d{1,2}$', text)
                        if text != link_text and text not in ['ÁôªÂΩï', 'Ê≥®ÂÜå', 'Home', 'album'] and not is_small_number:
                            product_name = text
                            break
        
        # Si a√∫n no tenemos nombre, usar el texto del enlace como √∫ltimo recurso
        if not product_name:
            link_text = link.get_text(strip=True)
            # Solo usar si no es un n√∫mero muy peque√±o (probablemente es el conteo de fotos)
            if link_text and not re.match(r'^\d{1,2}$', link_text) and len(link_text) > 0:
                product_name = link_text
        
        # Filtrar nombres inv√°lidos y verificar que tenga sentido
        if product_name and len(product_name) >= 1:
            # Filtrar nombres de navegaci√≥n y URLs
            invalid = ['ÁôªÂΩï', 'Ê≥®ÂÜå', 'Home', 'album', 'All categories', 'Yupoo', 'search', 'QR code', 'ÁÆÄ‰Ωì‰∏≠Êñá', 'english']
            # Aceptar cualquier nombre que no est√© en la lista de inv√°lidos
            if (product_name not in invalid and 
                not product_name.startswith('http') and 
                len(product_name.strip()) >= 1):
                seen_album_ids.add(album_id)
                seen_urls.add(full_url)
                products.append({
                    'url': full_url,
                    'name': product_name.strip()
                })
    
    # Eliminar duplicados por nombre (por si acaso)
    seen_names = {}  # Cambiar a dict para guardar el primer producto con ese nombre
    unique_products = []
    duplicates_info = []  # Lista de duplicados con informaci√≥n
    
    for product in products:
        product_name = product['name']
        if product_name not in seen_names:
            seen_names[product_name] = product  # Guardar el primer producto con ese nombre
            unique_products.append(product)
        else:
            # Es un duplicado - guardar informaci√≥n
            duplicates_info.append({
                'name': product_name,
                'url': product['url'],
                'first_url': seen_names[product_name]['url']  # URL del primer producto con ese nombre
            })
    
    return (unique_products, len(duplicates_info), duplicates_info)

def get_image_urls_from_product(product_url, session=None, retries=MAX_RETRIES):
    """Obtiene las URLs de todas las im√°genes de un producto, con reintentos.
    
    Returns:
        tuple: (list of image URLs, success: bool) - success es False si hubo un error al obtener las URLs
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': 'https://yitian333.x.yupoo.com/',
    }
    last_error = None
    
    for attempt in range(retries):
        try:
            if attempt > 0:
                wait_time = attempt * 2
                print(f"  ‚Üª Reintento {attempt + 1} de {retries} al obtener URLs (esperando {wait_time}s)...")
                time.sleep(wait_time)
            
            # Usar sesi√≥n si est√° disponible (para mantener cookies de autenticaci√≥n)
            if session:
                response = session.get(product_url, headers=headers, timeout=15)
            else:
                response = requests.get(product_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar im√°genes en el visor de galer√≠a
            image_urls = []
            
            # M√©todo 1: Buscar en elementos de imagen con clases espec√≠ficas
            img_elements = soup.find_all('img', class_=lambda x: x and ('image__img' in str(x) or 'showalbum__bigimg' in str(x)))
            for img in img_elements:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    # Convertir URLs protocol-relative a https
                    if src.startswith('//'):
                        src = 'https:' + src
                    # Solo procesar URLs v√°lidas de im√°genes de productos
                    if src.startswith('http') and 'photo.yupoo.com' in src:
                        # Limpiar par√°metros de tama√±o para obtener la imagen completa
                        src = re.sub(r'\?.*$', '', src)
                        src = src.replace('/small.', '/large.')
                        if src not in image_urls:
                            image_urls.append(src)
            
            # M√©todo 2: Buscar en divs con estilos de fondo
            divs_with_bg = soup.find_all('div', style=lambda x: x and 'background-image' in str(x))
            for div in divs_with_bg:
                style = div.get('style', '')
                match = re.search(r'url\(["\']?(https?://[^"\')]+|//[^"\')]+)["\']?\)', style)
                if match:
                    url = match.group(1)
                    if url.startswith('//'):
                        url = 'https:' + url
                    if 'photo.yupoo.com' in url:
                        url = re.sub(r'\?.*$', '', url)
                        url = url.replace('/small.', '/large.')
                        if url not in image_urls:
                            image_urls.append(url)
            
            # M√©todo 3: Buscar todas las im√°genes que parezcan ser de productos
            all_imgs = soup.find_all('img')
            for img in all_imgs:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    if src.startswith('//'):
                        src = 'https:' + src
                    # Solo im√°genes de photo.yupoo.com (productos reales)
                    if src.startswith('http') and 'photo.yupoo.com' in src:
                        src = re.sub(r'\?.*$', '', src)
                        src = src.replace('/small.', '/large.')
                        # Filtrar logos, static, website
                        if (src not in image_urls and 
                            '/static/' not in src and 
                            '/website/' not in src and
                            '/icons/' not in src):
                            image_urls.append(src)
            
            return (image_urls, True)  # Retornar tupla: (urls, success)
            
        except Exception as e:
            last_error = e
            if attempt < retries - 1:
                continue
            else:
                print(f"  ‚úó Error obteniendo im√°genes (tras {retries} intentos): {e}")
                return ([], False)  # Retornar lista vac√≠a y False (fall√≥)
    
    if last_error:
        print(f"  ‚úó Error obteniendo im√°genes (tras {retries} intentos): {last_error}")
    return ([], False)

def download_image(image_url, save_path, retries=MAX_RETRIES, session=None):
    """Descarga una imagen con reintentos y manejo robusto de errores de conexi√≥n"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://yitian333.x.yupoo.com/',
    }
    
    for attempt in range(retries):
        try:
            # Usar sesi√≥n si est√° disponible
            if session:
                response = session.get(image_url, headers=headers, timeout=30, stream=True)
            else:
                response = requests.get(image_url, headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # Descargar en chunks con manejo de errores de conexi√≥n
            with open(save_path, 'wb') as f:
                try:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                except (requests.exceptions.ChunkedEncodingError, ConnectionError) as chunk_error:
                    # Error durante la descarga de chunks (IncompleteRead, etc.)
                    # Verificar si el archivo tiene contenido v√°lido
                    f.flush()
                    if save_path.exists() and save_path.stat().st_size > 0:
                        # El archivo tiene contenido, puede ser una imagen parcial
                        # Intentar verificar si es v√°lida o reintentar
                        pass
                    raise chunk_error
            
            # Verificar que el archivo se descarg√≥ completamente
            if save_path.exists() and save_path.stat().st_size > 0:
                return True
            else:
                raise Exception("Archivo descargado est√° vac√≠o")
            
        except (requests.exceptions.ChunkedEncodingError, 
                requests.exceptions.ConnectionError,
                ConnectionError,
                requests.exceptions.Timeout) as e:
            # Errores de conexi√≥n: reintentar con m√°s tiempo de espera
            if attempt < retries - 1:
                wait_time = (attempt + 1) * 2  # Esperar m√°s tiempo en cada reintento
                time.sleep(wait_time)
                # Eliminar archivo parcial si existe
                if save_path.exists():
                    try:
                        save_path.unlink()
                    except:
                        pass
                continue
            else:
                print(f"  ‚úó Error de conexi√≥n descargando {image_url}: {e}")
                if save_path.exists():
                    try:
                        save_path.unlink()  # Eliminar archivo parcial
                    except:
                        pass
                return False
        except Exception as e:
            if attempt < retries - 1:
                time.sleep(1)
                # Eliminar archivo parcial si existe
                if save_path.exists():
                    try:
                        save_path.unlink()
                    except:
                        pass
                continue
            else:
                print(f"  ‚úó Error descargando {image_url}: {e}")
                if save_path.exists():
                    try:
                        save_path.unlink()  # Eliminar archivo parcial
                    except:
                        pass
                return False
    
    return False

def process_product(product, base_dir, page_num, session=None):
    """Procesa un producto: descarga sus im√°genes
    
    Returns:
        tuple: (n√∫mero de im√°genes descargadas, √©xito: bool) - √©xito es False si fall√≥ obtener las URLs
    """
    product_name = sanitize_filename(product['name'])
    product_url = product['url']
    
    # Crear directorio del producto
    product_dir = base_dir / str(page_num) / product_name
    product_dir.mkdir(parents=True, exist_ok=True)
    
    # Obtener URLs de im√°genes
    image_urls, success = get_image_urls_from_product(product_url, session)
    
    if not image_urls:
        if not success:
            # Hubo un error al obtener las URLs (timeout, etc.)
            print("  ‚ö† No se encontraron im√°genes (error al obtener URLs)")
            return (0, False)
        else:
            # No hay im√°genes pero no hubo error (producto sin im√°genes)
            print("  ‚ö† No se encontraron im√°genes")
            return (0, True)
    
    # Descargar cada imagen
    downloaded = 0
    for idx, img_url in enumerate(image_urls, 1):
        # Extraer nombre original de la URL
        # URL t√≠pica: https://photo.yupoo.com/yitian333/5073f2ba/large.jpeg
        url_parts = img_url.rstrip('/').split('/')
        if len(url_parts) >= 2:
            # Obtener el ID √∫nico y el nombre del archivo
            image_id = url_parts[-2]  # ej: 5073f2ba
            original_filename = url_parts[-1]  # ej: large.jpeg
            
            # Extraer extensi√≥n del nombre original
            if '.' in original_filename:
                ext = '.' + original_filename.split('.')[-1]
            else:
                ext = '.jpg'
            
            # Usar el ID como nombre (es √∫nico y descriptivo)
            filename = image_id + ext
        else:
            # Fallback: usar √≠ndice si no se puede extraer
            ext = '.jpg'
            if '.png' in img_url.lower():
                ext = '.png'
            elif '.webp' in img_url.lower():
                ext = '.webp'
            filename = "{}{}".format(idx, ext)
        
        image_path = product_dir / filename
        
        # Verificar si ya existe (con cualquier extensi√≥n .jpg/.jpeg)
        if image_path.exists():
            downloaded += 1
            continue
        
        # Tambi√©n verificar la extensi√≥n alternativa para evitar duplicados
        if ext.lower() == '.jpeg':
            alt_path = product_dir / (image_id + '.jpg')
        elif ext.lower() == '.jpg':
            alt_path = product_dir / (image_id + '.jpeg')
        else:
            alt_path = None
        
        if alt_path and alt_path.exists():
            # Ya existe con otra extensi√≥n, saltar
            downloaded += 1
            continue
        
        # Descargar
        if download_image(img_url, image_path, session=session):
            downloaded += 1
        
        # Peque√±a pausa entre im√°genes
        time.sleep(DELAY_BETWEEN_IMAGES)
    
    print(f"  ‚úì Descargadas {downloaded} im√°genes")
    return (downloaded, True)  # Retornar tupla: (im√°genes descargadas, √©xito)

def main(base_url=None, category_name=None, start_page=None, end_page=None, password=None):
    """Funci√≥n principal"""
    # Usar argumentos o valores por defecto
    base_url = base_url or BASE_URL
    start_page = start_page or START_PAGE
    end_page = end_page or END_PAGE
    password = password or PASSWORD
    
    print("="*70)
    print("Descargador de im√°genes Yupoo por categor√≠a")
    print("="*70)
    print(f"URL base: {base_url}")
    print(f"P√°ginas: {start_page} a {end_page}")
    if password:
        print(f"Contrase√±a: {'*' * len(password)} (configurada)")
    print("="*70)
    print()
    
    # Crear sesi√≥n para mantener cookies de autenticaci√≥n
    session = requests.Session()
    
    # Headers para las peticiones
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': 'https://yitian333.x.yupoo.com/',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
    }
    session.headers.update(headers)
    
    # Autenticar si es necesario
    if password:
        print("Verificando autenticaci√≥n...")
        if not authenticate_if_needed(session, base_url, password):
            print("‚úó Error: No se pudo autenticar. Verifica la contrase√±a.")
            return
        print("‚úì Autenticaci√≥n exitosa")
        print()
    
    # Obtener nombre de categor√≠a
    if not category_name:
        print("Extrayendo nombre de categor√≠a...")
        try:
            response = session.get(f"{base_url}?page=1", timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            category_name = extract_category_name(soup, base_url)
            
            if category_name:
                print(f"‚úì Categor√≠a detectada: {category_name}")
            else:
                # Usar ID de categor√≠a como respaldo
                match = re.search(r'/categories/(\d+)', base_url)
                if match:
                    category_name = f"Categoria_{match.group(1)}"
                    print(f"‚ö† No se pudo extraer nombre, usando: {category_name}")
                else:
                    category_name = "Yupoo_Downloads"
                    print(f"‚ö† Usando nombre por defecto: {category_name}")
        except Exception as e:
            print(f"‚úó Error extrayendo categor√≠a: {e}")
            category_name = "Yupoo_Downloads"
            print(f"‚ö† Usando nombre por defecto: {category_name}")
    
    # Sanitizar nombre de categor√≠a
    category_name_clean = sanitize_filename(category_name)
    
    print()
    print(f"Categor√≠a: {category_name}")
    print("="*70)
    print()
    
    # Crear directorio base
    base_dir = Path("yupoo_downloads") / category_name_clean
    base_dir.mkdir(parents=True, exist_ok=True)
    
    # Estad√≠sticas
    total_products = 0
    total_images = 0
    failed_products = 0
    total_duplicates = 0  # Contador de productos duplicados detectados
    all_duplicates = []  # Lista de todos los duplicados detectados
    cross_page_duplicates = []  # Duplicados entre p√°ginas diferentes
    
    # Rastrear productos ya procesados entre todas las p√°ginas
    processed_products = {}  # {product_name: {'page': page_num, 'dir': Path}}
    
    # Procesar cada p√°gina
    for page in range(start_page, end_page + 1):
        print()
        print("="*70)
        print(f"Procesando p√°gina {page}...")
        print("="*70)
        
        try:
            # Obtener HTML de la p√°gina
            page_url = f"{base_url}?page={page}"
            response = session.get(page_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Verificar si a√∫n requiere autenticaci√≥n (por si expir√≥ la sesi√≥n)
            if password and is_password_protected(soup):
                print("‚ö† Sesi√≥n expirada, reautenticando...")
                if not authenticate_if_needed(session, base_url, password):
                    print("‚úó Error: No se pudo reautenticar")
                    continue
                # Reintentar la petici√≥n
                response = session.get(page_url, timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extraer productos
            products, duplicates_count, duplicates_info = extract_products_from_page(soup, base_url)
            total_duplicates += duplicates_count
            all_duplicates.extend(duplicates_info)
            if duplicates_count > 0:
                print(f"Encontrados {len(products)} productos √∫nicos en p√°gina {page} ({duplicates_count} duplicados detectados y omitidos)")
            else:
                print(f"Encontrados {len(products)} productos en p√°gina {page}")
            print()
            
            # Procesar cada producto
            for idx, product in enumerate(products, 1):
                total_products += 1
                product_name = sanitize_filename(product['name'])
                print(f"[{idx}/{len(products)}] Producto: {product['name']}")
                print(f"  URL: {product['url']}")
                
                # Verificar si este producto ya fue procesado en una p√°gina anterior
                if product_name in processed_products:
                    prev_info = processed_products[product_name]
                    prev_page = prev_info['page']
                    prev_dir = prev_info['dir']
                    
                    # Verificar si el directorio anterior tiene im√°genes
                    if prev_dir.exists() and any(prev_dir.iterdir()):
                        print(f"  üîÑ Duplicado detectado: ya descargado en p√°gina {prev_page}")
                        print(f"     Consolidando im√°genes nuevas en carpeta de p√°gina {prev_page}...")
                        
                        # Obtener URLs de im√°genes del producto actual
                        image_urls, success = get_image_urls_from_product(product['url'], session)
                        
                        if not success or not image_urls:
                            print(f"     ‚ö† No se pudieron obtener im√°genes del duplicado, omitiendo")
                            cross_page_duplicates.append({
                                'name': product['name'],
                                'current_page': page,
                                'current_url': product['url'],
                                'previous_page': prev_page,
                                'previous_dir': str(prev_dir),
                                'images_added': 0
                            })
                            continue
                        
                        # Obtener IDs de im√°genes ya descargadas
                        existing_image_ids = set()
                        for img_file in prev_dir.glob("*"):
                            if img_file.is_file() and img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                                existing_image_ids.add(img_file.stem)  # ID sin extensi√≥n
                        
                        # Descargar solo im√°genes nuevas
                        new_images_count = 0
                        for img_url in image_urls:
                            # Extraer ID de imagen de la URL
                            url_parts = img_url.rstrip('/').split('/')
                            if len(url_parts) >= 2:
                                image_id = url_parts[-2]
                                original_filename = url_parts[-1]
                                
                                if '.' in original_filename:
                                    ext = '.' + original_filename.split('.')[-1]
                                else:
                                    ext = '.jpg'
                                
                                filename = image_id + ext
                            else:
                                continue
                            
                            # Si la imagen ya existe, saltar
                            image_path = prev_dir / filename
                            if image_path.exists():
                                continue
                            
                            # Verificar extensi√≥n alternativa
                            if ext.lower() == '.jpeg':
                                alt_path = prev_dir / (image_id + '.jpg')
                            elif ext.lower() == '.jpg':
                                alt_path = prev_dir / (image_id + '.jpeg')
                            else:
                                alt_path = None
                            
                            if alt_path and alt_path.exists():
                                continue
                            
                            # Descargar imagen nueva en la carpeta de la primera p√°gina
                            if download_image(img_url, image_path, session=session):
                                new_images_count += 1
                                total_images += 1
                                time.sleep(DELAY_BETWEEN_IMAGES)
                        
                        if new_images_count > 0:
                            print(f"     ‚úì Agregadas {new_images_count} im√°genes nuevas (total: {len(existing_image_ids) + new_images_count})")
                        else:
                            print(f"     ‚Ñπ No hay im√°genes nuevas, todas ya est√°n descargadas")
                        
                        cross_page_duplicates.append({
                            'name': product['name'],
                            'current_page': page,
                            'current_url': product['url'],
                            'previous_page': prev_page,
                            'previous_dir': str(prev_dir),
                            'images_added': new_images_count
                        })
                        continue
                
                try:
                    images, success = process_product(product, base_dir, page, session)
                    total_images += images
                    
                    # Registrar este producto como procesado solo si fue exitoso y tiene im√°genes
                    if success and images > 0:
                        product_dir = base_dir / str(page) / product_name
                        processed_products[product_name] = {
                            'page': page,
                            'dir': product_dir
                        }
                    
                    if not success:
                        # Fall√≥ al obtener las URLs de im√°genes (timeout, error de conexi√≥n, etc.)
                        failed_products += 1
                except Exception as e:
                    print(f"  ‚úó Error procesando producto: {e}")
                    failed_products += 1
                
                # Pausa entre productos
                time.sleep(DELAY_BETWEEN_REQUESTS)
            
        except Exception as e:
            print(f"‚úó Error procesando p√°gina {page}: {e}")
    
    # Resumen final
    print()
    print("="*70)
    print("RESUMEN FINAL")
    print("="*70)
    print(f"Total productos procesados: {total_products}")
    print(f"  ‚úì Exitosos: {total_products - failed_products}")
    print(f"  ‚úó Fallidos: {failed_products}")
    if total_duplicates > 0:
        print(f"  üîÑ Duplicados en misma p√°gina (omitidos): {total_duplicates}")
    if cross_page_duplicates:
        print(f"  üîÑ Duplicados entre p√°ginas (omitidos): {len(cross_page_duplicates)}")
    
    # Mostrar detalles de duplicados
    if total_duplicates > 0 or cross_page_duplicates:
        print()
        print("  Productos duplicados:")
        
        # Duplicados en la misma p√°gina
        if total_duplicates > 0:
            from collections import defaultdict
            duplicates_by_name = defaultdict(list)
            for dup in all_duplicates:
                duplicates_by_name[dup['name']].append(dup)
            
            for name, dup_list in sorted(duplicates_by_name.items()):
                print(f"    ‚Ä¢ \"{name}\" (duplicado en misma p√°gina):")
                print(f"      - Descargado: {dup_list[0]['first_url']}")
                for dup in dup_list:
                    print(f"      - Omitido: {dup['url']}")
        
        # Duplicados entre p√°ginas
        if cross_page_duplicates:
            from collections import defaultdict
            cross_duplicates_by_name = defaultdict(list)
            for dup in cross_page_duplicates:
                cross_duplicates_by_name[dup['name']].append(dup)
            
            for name, dup_list in sorted(cross_duplicates_by_name.items()):
                print(f"    ‚Ä¢ \"{name}\" (duplicado entre p√°ginas):")
                # Agrupar por p√°gina anterior
                for dup in dup_list:
                    print(f"      - Ya descargado en p√°gina {dup['previous_page']}: {dup['previous_dir']}")
                    print(f"      - Omitido en p√°gina {dup['current_page']}: {dup['current_url']}")
    
    print()
    print(f"Total im√°genes descargadas: {total_images}")
    print(f"üìÅ Ubicaci√≥n: {base_dir.absolute()}")
    print("="*70)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Descarga im√°genes de una categor√≠a Yupoo.")
    parser.add_argument('--url', type=str, help='URL base de la categor√≠a Yupoo.')
    parser.add_argument('--name', type=str, default=None, help='Nombre de la categor√≠a (opcional, se extraer√° si no se especifica).')
    parser.add_argument('--start', type=int, help='P√°gina inicial a procesar.')
    parser.add_argument('--end', type=int, help='P√°gina final a procesar.')
    parser.add_argument('--password', type=str, default=None, help='Contrase√±a para p√°ginas protegidas (opcional).')
    
    args = parser.parse_args()
    
    main(args.url, args.name, args.start, args.end, args.password)
